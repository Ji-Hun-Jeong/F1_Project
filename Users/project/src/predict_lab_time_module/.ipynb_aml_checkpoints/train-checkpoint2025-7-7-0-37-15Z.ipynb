{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3c8b21ab",
      "metadata": {
        "gather": {
          "logged": 1753626946554
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from Users.project.predict_lab_time_module.predict_lap_time import PredictLapTime\n",
        "from Users.project.data_container.data_container import AzureStorageAccess, FolderAccess\n",
        "from Users.project.train_process import model_predictor\n",
        "from Users.project.train_process.data_state import DataFeaturing, IDatasetCreator\n",
        "from Users.project.train_process.file_name_storage import FileNameStorage\n",
        "from Users.project.train_process.loss_func import ILossFunc, MSELoss\n",
        "from Users.project.train_process.model import ImprovedNN, Model\n",
        "from Users.project.train_process.model_loader import ModelLoader\n",
        "from Users.project.train_process.model_predictor import IModelPredictor, MyModelPredictor\n",
        "from Users.project.train_process.model_storage import ModelStorage\n",
        "from Users.project.train_process.model_trainer import IModelTrainer, MyModelTrainer\n",
        "from Users.project.train_process.my_utils import extract_track_from_path, get_real_track_name\n",
        "from Users.project.train_process.optimizer import AdamOptimizer\n",
        "from Users.project.train_process.process_unit import ProcessUnit\n",
        "from Users.project.predict_lab_time_module.create_lap_time_dataset import LapTimePredictDatasetCreator, EFeatureType\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "data_access = AzureStorageAccess()\n",
        "featuring = DataFeaturing()\n",
        "dataset_creator = LapTimePredictDatasetCreator()\n",
        "game_path = \"./Users/project/correct_file/\"\n",
        "log_path = \"./Users/project/log/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bcaa3d44",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# weather_data = None\n",
        "# car_data = None\n",
        "# laps_data = None\n",
        "\n",
        "# car_file_name = None\n",
        "# laps_file_name = None\n",
        "# for file in data_access.get_all_file():\n",
        "#     file_name = file.name\n",
        "\n",
        "#     tokens = file_name.split(\"/\")\n",
        "#     current_base_game = tokens[1]\n",
        "\n",
        "#     if len(tokens) < 4:\n",
        "#         continue\n",
        "\n",
        "#     file_format = tokens[3]\n",
        "#     if file_format == \"car_data_all.csv\":\n",
        "#         cs = file_name\n",
        "#         car_file_name = f\"{tokens[1]}/{tokens[2]}\"\n",
        "#         car_data = data_access.read_csv_from_blob(file_name)\n",
        "#         check_value = 1\n",
        "#     elif file_format == \"laps.csv\" and check_value == 1:\n",
        "#         ls = file_name\n",
        "#         laps_file_name = f\"{tokens[1]}/{tokens[2]}\"\n",
        "#         laps_data = data_access.read_csv_from_blob(file_name)\n",
        "\n",
        "#         if car_file_name != laps_file_name:\n",
        "#             print(\"Warning: \", car_file_name, laps_file_name)\n",
        "#         ws = f\"{tokens[0]}/{tokens[1]}/weather_data.csv\"\n",
        "#         weather_data = data_access.read_csv_by_data_frame(ws)\n",
        "#         if 5 < len(car_data) and 2 < len(laps_data) and (car_data.isnull().values.any() == False) and (weather_data.isnull().values.any() == False):\n",
        "#             game_name = get_real_track_name(extract_track_from_path(file_name))\n",
        "#             print(game_name)\n",
        "#             with open(game_path + game_name, \"a\", encoding=\"utf-8\") as file:\n",
        "#                 file.write(f\"{tokens[0]}/{tokens[1]}/{tokens[2]}/\\n\")\n",
        "\n",
        "#         check_value = 0\n",
        "#     print(check_value)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ed7c8c9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_weather_data(current_time, data_frame: pd.DataFrame, debug_log: str):\n",
        "    last_data = data_frame[data_frame[\"Time\"] <= current_time].tail(1)\n",
        "    return last_data\n",
        "\n",
        "\n",
        "def get_car_data_feature(data_frame: pd.DataFrame) -> list[float]:\n",
        "    # 데이터 추출\n",
        "    rpm_datas = data_frame[\"RPM\"]\n",
        "    speed_datas = data_frame[\"Speed\"]\n",
        "    gear_datas = data_frame[\"nGear\"]\n",
        "    throttle_datas = data_frame[\"Throttle\"]\n",
        "    brake_datas = data_frame[\"Brake\"]\n",
        "    drs_datas = data_frame[\"DRS\"]\n",
        "\n",
        "    # 피처 리스트 초기화\n",
        "    features = []\n",
        "\n",
        "    # 1. RPM 피처\n",
        "    features.append(rpm_datas.mean())  # 평균 RPM\n",
        "    features.append(rpm_datas.max())   # 최대 RPM\n",
        "    features.append(rpm_datas.diff().abs().mean())  \n",
        "    features.append(rpm_datas.std())   # RPM 표준편차 (변동성)\n",
        "\n",
        "    # 2. Speed 피처\n",
        "    features.append(speed_datas.mean())  # 평균 속도\n",
        "    features.append(speed_datas.max())   # 최대 속도\n",
        "    features.append(speed_datas.diff().abs().mean())  # 평균 속도 변화율 (가속도)\n",
        "    features.append(speed_datas.std())   # 속도 표준편차\n",
        "\n",
        "    # 3. nGear 피처\n",
        "    features.append(gear_datas.mean()) \n",
        "    features.append(gear_datas.max())  \n",
        "    features.append(gear_datas.diff().abs().mean())  # 평균 기어 변화율\n",
        "    features.append(gear_datas.std()) \n",
        "    features.append(gear_datas.value_counts().max())  # 가장 많이 사용된 기어의 빈도\n",
        "    features.append((gear_datas.shift() != gear_datas)[1:].sum())  # 기어 전환 횟수\n",
        "\n",
        "    # 4. Throttle 피처\n",
        "    features.append(throttle_datas.mean())  # 평균 스로틀 사용\n",
        "    features.append(throttle_datas.max())   \n",
        "    features.append(throttle_datas.diff().abs().mean()) \n",
        "    features.append(throttle_datas.std()) \n",
        "    features.append((throttle_datas > 0.95).sum() / len(throttle_datas)) # 거의 풀 스로틀의 비율\n",
        "\n",
        "    # 5. Brake 피처\n",
        "    features.append(brake_datas.mean())  # 브레이크 사용 비율\n",
        "\n",
        "    # 6. DRS 피처\n",
        "    features.append(drs_datas.mean())  # DRS 평균\n",
        "    features.append(drs_datas.max())  # DRS 최대\n",
        "    features.append(drs_datas.diff().abs().mean())  # 평균 drs 변화율\n",
        "    features.append(drs_datas.std())  \n",
        "    features.append(drs_datas.value_counts().max())  # 가장 많이 사용된 drs 빈도\n",
        "    features.append((drs_datas.shift() != drs_datas)[1:].sum())  # drs 전환 횟수\n",
        "\n",
        "#       코스팅(Coasting) 시간 비율: 스로틀과 브레이크를 모두 사용하지 않는 타력 주행 구간의 비율입니다. 드라이버의 효율성을 나타내는 지표가 될 수 있습니다.\n",
        "#       # Throttle과 Brake가 모두 5% 미만인 시간의 비율\n",
        "#       ((throttle_datas < 0.05) & (brake_datas < 0.05)).sum() / len(data_frame)\n",
        "    return features\n",
        "\n",
        "def get_laps_data_feature(row: pd.DataFrame) -> list[float]:\n",
        "    sector1_time = pd.to_timedelta(row.Sector1Time).total_seconds()\n",
        "    sector2_time = pd.to_timedelta(row.Sector2Time).total_seconds()\n",
        "    sector3_time = pd.to_timedelta(row.Sector3Time).total_seconds()\n",
        "    compound = row.Compound\n",
        "    fresh_tyre = row.FreshTyre\n",
        "    track_status = int(row.TrackStatus)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    # 타이어 상태 원핫인코딩\n",
        "    tyre_mapping = {\"SOFT\": [1, 0, 0], \"MEDIUM\": [0, 1, 0], \"HARD\": [0, 0, 1]}\n",
        "    compound_encoded = tyre_mapping.get(compound, [1, 0, 0])  # 알 수 없는 타이어는 [0,0,0]\n",
        "\n",
        "    track_flags = []\n",
        "    flag = 16  # 2^4부터 시작 (5비트)\n",
        "    while flag != 0:\n",
        "        track_flags.append(1.0 if track_status & flag else 0.0)\n",
        "        flag //= 2\n",
        "\n",
        "\n",
        "    features.append(fresh_tyre)\n",
        "    features.append(sector1_time)\n",
        "    features.append(sector2_time)\n",
        "    features.append(sector3_time)\n",
        "    features += compound_encoded\n",
        "    features += track_flags\n",
        "\n",
        "    return features\n",
        "\n",
        "def get_weather_data_feature(data_frame: pd.DataFrame) -> list[float]:\n",
        "    air_temp = data_frame[\"AirTemp\"].item()\n",
        "    humidity = data_frame[\"Humidity\"].item()\n",
        "    pressure = data_frame[\"Pressure\"].item()\n",
        "    rainfall = data_frame[\"Rainfall\"].item()\n",
        "    track_temp = data_frame[\"TrackTemp\"].item()\n",
        "    wind_direction = data_frame[\"WindDirection\"].item()\n",
        "    wind_speed = data_frame[\"WindSpeed\"].item()\n",
        "    features = [air_temp, humidity, pressure, rainfall, track_temp, wind_direction, wind_speed]\n",
        "    return features\n",
        "    \n",
        "def analysis_one_team(car_data: pd.DataFrame, laps_data: pd.DataFrame, weather_data: pd.DataFrame, debug_log: str):\n",
        "    # Time 컬럼을 timedelta로 변환\n",
        "    car_data[\"Time\"] = pd.to_timedelta(car_data[\"Time\"])\n",
        "    laps_data[\"LapTime\"] = pd.to_timedelta(laps_data[\"LapTime\"])\n",
        "    weather_data[\"Time\"] = pd.to_timedelta(weather_data[\"Time\"])\n",
        "    progress_time = pd.Timedelta(0)\n",
        "\n",
        "    one_team_features = []\n",
        "    one_team_label = []\n",
        "\n",
        "    for row in laps_data.itertuples():\n",
        "        lap_number = row.LapNumber\n",
        "        is_accurate = row.IsAccurate\n",
        "\n",
        "        if is_accurate == False:\n",
        "            continue\n",
        "\n",
        "        lap_time = row.LapTime\n",
        "        progress_time += lap_time\n",
        "\n",
        "        same_lap_number_data = car_data[car_data[\"LapNumber\"] == lap_number]\n",
        "\n",
        "        if same_lap_number_data.empty or len(same_lap_number_data) < 2:\n",
        "            continue\n",
        "\n",
        "        weather_frame = get_weather_data(progress_time, weather_data, debug_log)\n",
        "\n",
        "        if weather_frame.empty:\n",
        "            continue\n",
        "\n",
        "        one_lap_features = []\n",
        "        car_data_feature = get_car_data_feature(same_lap_number_data)\n",
        "        laps_data_feature = get_laps_data_feature(row)\n",
        "        weather_data_feature = get_weather_data_feature(weather_frame)\n",
        "\n",
        "        one_lap_features += car_data_feature\n",
        "        one_lap_features += laps_data_feature\n",
        "        one_lap_features += weather_data_feature\n",
        "        one_lap_features = [float(x) if isinstance(x, (np.float32, np.float64)) else x for x in one_lap_features]\n",
        "        one_lap_features = [int(x) if isinstance(x, (np.int32, np.int64)) else x for x in one_lap_features]\n",
        "        one_team_features.append(one_lap_features)\n",
        "\n",
        "        one_team_label.append([lap_time.total_seconds()])\n",
        "    return one_team_features, one_team_label\n",
        "\n",
        "feature_headers = [\n",
        "    # 1. RPM\n",
        "    \"rpm_mean\", \"rpm_max\", \"rpm_diff_mean\", \"rpm_std\",\n",
        "    \n",
        "    # 2. Speed\n",
        "    \"speed_mean\", \"speed_max\", \"speed_diff_mean\", \"speed_std\",\n",
        "    \n",
        "    # 3. nGear\n",
        "    \"gear_mean\", \"gear_max\", \"gear_diff_mean\", \"gear_std\",\n",
        "    \"gear_most_freq\", \"gear_change_count\",\n",
        "    \n",
        "    # 4. Throttle\n",
        "    \"throttle_mean\", \"throttle_max\", \"throttle_diff_mean\", \"throttle_std\",\n",
        "    \"throttle_full_ratio\",\n",
        "    \n",
        "    # 5. Brake\n",
        "    \"brake_mean\",\n",
        "    \n",
        "    # 6. DRS\n",
        "    \"drs_mean\", \"drs_max\", \"drs_diff_mean\", \"drs_std\",\n",
        "    \"drs_most_freq\", \"drs_change_count\",\n",
        "    \n",
        "    # Laps Feature\n",
        "    \"fresh_tyre\",\n",
        "    \"sector1_time\", \"sector2_time\", \"sector3_time\",\n",
        "    \"tyre_SOFT\", \"tyre_MEDIUM\", \"tyre_HARD\",\n",
        "    \"flag_16\", \"flag_8\", \"flag_4\", \"flag_2\", \"flag_1\",  # 5비트\n",
        "\n",
        "    # Weather Feature\n",
        "    \"air_temp\", \"humidity\", \"pressure\", \"rainfall\", \"track_temp\",\n",
        "    \"wind_direction\", \"wind_speed\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c40c1397",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "class MyNN(nn.Module):\n",
        "    def __init__(self, in_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(64, 16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "116c1dc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "def train(track_name: str):\n",
        "    with open(\n",
        "        game_path + track_name, \"r\", encoding=\"utf-8\"\n",
        "    ) as track_game_team_list_file:\n",
        "        one_game_features = []\n",
        "        one_game_labels = []\n",
        "        for track_game_team_str in track_game_team_list_file:\n",
        "            game_team = track_game_team_str.strip()\n",
        "            tokens = game_team.split(\"/\")\n",
        "\n",
        "            car_data = data_access.read_csv_by_data_frame(\n",
        "                game_team + \"car_data_all.csv\"\n",
        "            )\n",
        "            laps_data = data_access.read_csv_by_data_frame(game_team + \"laps.csv\")\n",
        "            weather_data = data_access.read_csv_by_data_frame(\n",
        "                f\"{tokens[0]}/{tokens[1]}/weather_data.csv\"\n",
        "            )\n",
        "            one_team_features, one_team_label = analysis_one_team(\n",
        "                car_data, laps_data, weather_data, game_team\n",
        "            )\n",
        "\n",
        "            one_game_features += one_team_features\n",
        "            one_game_labels += one_team_label\n",
        "\n",
        "        # 파이프라인 1: 데이터 피쳐링 후 파일로 저장\n",
        "        path = os.path.join(log_path, track_name, \"features.csv\")\n",
        "        df = pd.DataFrame(one_game_features)\n",
        "        if df.isnull().values.any():\n",
        "            print(f\"{track_name} in nan\")\n",
        "            return False\n",
        "\n",
        "        df.to_csv(path, index=False, header=feature_headers)\n",
        "\n",
        "        # 파이프라인 2: 스케일링\n",
        "        standard_scaler = StandardScaler()\n",
        "        scaled_array = standard_scaler.fit_transform(df)  # numpy배열로 리턴\n",
        "        df = pd.DataFrame(scaled_array)\n",
        "        path = os.path.join(log_path, track_name, \"scaling.csv\")\n",
        "        df.to_csv(path, index=False, header=feature_headers)\n",
        "\n",
        "        # 각 트랙에 맞는 모델을 만들고 학습시키기\n",
        "        one_game_labels = pd.DataFrame(one_game_labels)\n",
        "        # --- 3. 학습/검증 데이터 분리 및 텐서 변환 ---\n",
        "        # numpy 배열을 사용하여 데이터를 분리\n",
        "        x_train, x_val, y_train, y_val = train_test_split(\n",
        "            df, one_game_labels.values, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # PyTorch 텐서로 변환\n",
        "        x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "        x_val_tensor = torch.tensor(x_val.values, dtype=torch.float32)\n",
        "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "        neural_network = MyNN(df.shape[1])\n",
        "        optimizer = optim.Adam(neural_network.parameters(), lr=0.01)\n",
        "        loss_func = nn.MSELoss()\n",
        "        num_epochs = 1000\n",
        "\n",
        "        path = os.path.join(log_path, track_name, \"loss.txt\")\n",
        "        f = open(path, 'w', encoding=\"utf-8\")\n",
        "        print(track_name)\n",
        "        for epoch in range(num_epochs):\n",
        "            # 학습 모드\n",
        "            neural_network.train()\n",
        "\n",
        "            # 순전파s\n",
        "            outputs = neural_network(x_train_tensor)\n",
        "            loss = loss_func(outputs, y_train_tensor)\n",
        "\n",
        "            # 역전파 및 최적화\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 검증 모드\n",
        "            neural_network.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = neural_network(x_val_tensor)\n",
        "                val_loss = loss_func(val_outputs, y_val_tensor)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                f.write(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\\n\")\n",
        "        f.close()\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48539e13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for track_name in os.listdir(game_path):\n",
        "#     train(track_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6a1a8f61",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qatar\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train(\"Qatar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455a241e",
      "metadata": {},
      "outputs": [],
      "source": [
        "s = set()\n",
        "\n",
        "for file in data_access.get_all_file():\n",
        "    if \"laps.csv\" not in file.name:\n",
        "        continue\n",
        "    file_name = file.name\n",
        "    tokens = file_name.split(\"/\")\n",
        "    if len(tokens) < 4:\n",
        "        continue\n",
        "    print(file.name)\n",
        "    df = data_access.read_csv_from_blob(file.name)\n",
        "\n",
        "    # isaccurate == True인 행만 필터링\n",
        "    df_filtered = df[df[\"IsAccurate\"] == True]\n",
        "\n",
        "    # NaN이 하나라도 있는 컬럼들만 추출\n",
        "    nan_columns = df_filtered.columns[df_filtered.isnull().any()].tolist()\n",
        "\n",
        "    # set에 추가\n",
        "    s.update(nan_columns)\n",
        "\n",
        "print(\"NaN이 포함된 컬럼들 (isaccurate=True 기준):\", s)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
